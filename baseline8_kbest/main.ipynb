{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.functions.preprocessing import clean, labelEncoder\n",
    "from Models.functions.datasets import loadTrainTest\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_regression, f_classif\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    features_maps = [50,50],\n",
    "    kernel_size = [3,4],\n",
    "    strides = [1,1],\n",
    "    dropout_rate = 0.3,\n",
    "    epochs = 100,\n",
    "    batch_size = 32,\n",
    "    embedding_dim = 100,\n",
    "    max_seq_length = None,\n",
    "    max_num_words = 150000,\n",
    "    pool_size = [2,2,2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "# Synthetic Minority Oversampling Technique (SMOTE)\n",
    "def oversampling(X, y):\n",
    "    try:\n",
    "        X_resampled, y_resampled = SMOTE().fit_resample(X, y)\n",
    "    except:\n",
    "        X_resampled, y_resampled = X, y\n",
    "        \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "root= '/home/rafael/Dataframe/'\n",
    "dataset_name = 'brblogset'\n",
    "lang = 'pt'\n",
    "task = 'education'\n",
    "reglog = LogisticRegression(C=100, penalty='l2', multi_class='auto', class_weight='balanced', solver='liblinear')\n",
    "\n",
    "X, _, y, _ = loadTrainTest(task, dataset_name, root, lang)\n",
    "\n",
    "# small sample\n",
    "\n",
    "X = X.apply(clean, lang=lang)\n",
    "X = X.values # mandatory for pan13\n",
    "\n",
    "y, n_classes, classes_names = labelEncoder(y)    \n",
    "params['n_classes'] = n_classes\n",
    "\n",
    "max_length = np.max([len(x.split(\" \")) for x in X])\n",
    "mean_length = int(np.mean([len(x.split(\" \")) for x in X]))\n",
    "median_length = int(np.median([len(x.split(\" \")) for x in X]))\n",
    "\n",
    "def test(k_best_func):\n",
    "    # train    \n",
    "    print(k_best_func)\n",
    "    \n",
    "    X_resampled, y_resampled = oversampling(X, y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled)\n",
    "    # feature transform\n",
    "    vect = TfidfVectorizer(max_features=None, ngram_range=(1,1), analyzer='word').fit(X_train)\n",
    "\n",
    "    X_train = vect.transform(X_train)\n",
    "\n",
    "    clf = reglog.fit(X_train, y_train)       \n",
    "\n",
    "    # test\n",
    "    X_test = vect.transform(X_test)\n",
    "    predicted = clf.predict(X_test)\n",
    "    #print(classification_report(y_test, predicted))\n",
    "    print(\"without k-best {0}\".format(f1_score(y_test, predicted, average='weighted')))\n",
    "\n",
    "    predicted1 = []\n",
    "    expected1 = []\n",
    "\n",
    "    predicted2 = []\n",
    "    expected2 = []\n",
    "\n",
    "    max_features = 130000\n",
    "\n",
    "    steps = 10000\n",
    "\n",
    "    kvalues = [i for i in range(500, max_features, steps)]\n",
    "    kvalues.append('all')\n",
    "\n",
    "    for kvalue in kvalues:\n",
    "        K = StratifiedKFold(n_splits=3)    \n",
    "\n",
    "        # Cross validation KFolds\n",
    "        for train_index, test_index in K.split(X, y):\n",
    "\n",
    "            X_resampled, y_resampled = oversampling(X, y)\n",
    "\n",
    "            X_train, X_test = X_resampled[train_index], X_resampled[test_index]\n",
    "            y_train, y_test = y_resampled[train_index], y_resampled[test_index]\n",
    "\n",
    "            # feature transform\n",
    "            vect = TfidfVectorizer(max_features=max_features, ngram_range=(1,1), analyzer='word').fit(X_train)\n",
    "            X_train = vect.transform(X_train).toarray()\n",
    "\n",
    "            # feature selection\n",
    "            sel = SelectKBest(k_best_func,k=kvalue)\n",
    "            ft = sel.fit(X_train, y_train)\n",
    "            train_best = ft.transform(X_train)\n",
    "\n",
    "            # train\n",
    "            clf1 = reglog.fit(train_best, y_train)\n",
    "\n",
    "            # test\n",
    "            X_test = vect.transform(X_test).toarray()\n",
    "            test_best = ft.transform(X_test)\n",
    "            pred = clf1.predict(test_best)\n",
    "\n",
    "            expected1.extend(y_test)\n",
    "            predicted1.extend(pred)        \n",
    "\n",
    "        #print(classification_report(y_test, predicted))    \n",
    "        print(\"kvalue {0} f1score {1}\".format(kvalue, f1_score(expected1, predicted1, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function chi2 at 0x7f22c0518510>\n",
      "without k-best 0.38280478866235296\n",
      "kvalue 500 f1score 0.36243228170994485\n",
      "kvalue 10500 f1score 0.3788179664834903\n",
      "kvalue 20500 f1score 0.38332446176543183\n",
      "kvalue 30500 f1score 0.38937148578297487\n",
      "kvalue 40500 f1score 0.3931316886030413\n",
      "kvalue 50500 f1score 0.3957167902930736\n",
      "kvalue 60500 f1score 0.39734938819269866\n",
      "kvalue 70500 f1score 0.39834614394073586\n",
      "kvalue 80500 f1score 0.3989369960610064\n",
      "kvalue 90500 f1score 0.39979246092788334\n",
      "kvalue 100500 f1score 0.4005394316641394\n",
      "kvalue 110500 f1score 0.40124277605308617\n",
      "kvalue 120500 f1score 0.4021995714038674\n",
      "kvalue all f1score 0.40284021457716856\n"
     ]
    }
   ],
   "source": [
    "#chi2, f_regression, f_classif\n",
    "test(chi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function f_regression at 0x7f22c0518598>\n",
      "without k-best 0.4150201281659471\n",
      "kvalue 500 f1score 0.31741917310352735\n",
      "kvalue 10500 f1score 0.33463976606079837\n",
      "kvalue 20500 f1score 0.3340084522876956\n",
      "kvalue 30500 f1score 0.33238739110730703\n",
      "kvalue 40500 f1score 0.33299977628391614\n",
      "kvalue 50500 f1score 0.3378590248537846\n",
      "kvalue 60500 f1score 0.342481926128817\n",
      "kvalue 70500 f1score 0.3453567583497951\n",
      "kvalue 80500 f1score 0.34743937806250064\n",
      "kvalue 90500 f1score 0.35000670626963293\n",
      "kvalue 100500 f1score 0.352449079617735\n",
      "kvalue 110500 f1score 0.3551640366281456\n",
      "kvalue 120500 f1score 0.35949009648390157\n",
      "kvalue all f1score 0.3638403201460455\n"
     ]
    }
   ],
   "source": [
    "test(f_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function f_classif at 0x7f22c0518400>\n",
      "without k-best 0.38015688265534847\n",
      "kvalue 500 f1score 0.3576694641008894\n",
      "kvalue 10500 f1score 0.36562383204129456\n",
      "kvalue 20500 f1score 0.37213684598912466\n",
      "kvalue 30500 f1score 0.3776351693813369\n",
      "kvalue 40500 f1score 0.37856001402995737\n",
      "kvalue 50500 f1score 0.37723362155531176\n",
      "kvalue 60500 f1score 0.37668953813538003\n",
      "kvalue 70500 f1score 0.3769983126624702\n",
      "kvalue 80500 f1score 0.37793881766634163\n",
      "kvalue 90500 f1score 0.37947469331770434\n",
      "kvalue 100500 f1score 0.3813366872416895\n",
      "kvalue 110500 f1score 0.38255617196398906\n",
      "kvalue 120500 f1score 0.38468373757761226\n",
      "kvalue all f1score 0.3866670008439158\n"
     ]
    }
   ],
   "source": [
    "test(f_classif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_filename = r'/home/rafael/GDrive/Embeddings/word2vec/'+ ds_name +'_sg_'+ str(params['embedding_dim']) +'dim.model'\n",
    "# vectors_filename = r'/home/rafael/GDrive/Embeddings/en_wordvectors/wiki-news-300d-1M.vec'\n",
    "# vectors_filename = r'/home/rafael/GDrive/Embeddings/nilc/fasttext_pt_skip_s'+ str(params['embedding_dim']) +r'.txt'        \n",
    "# WINDOWS\n",
    "# vectors_filename = r'C:/Users/Rafael Sandroni/Google Drive/Mestrado/Data/Embeddings/fasttext/'+dataset_name+r'_sg_100dim.model'\n",
    "# vectors_filename = r'/home/rafael/GDrive/Embeddings/nilc/fasttext_pt_skip_s'+ str(params['embedding_dim']) +r'.txt'        \n",
    "embedding_type = 1\n",
    "\n",
    "embedding_layer = create_embeddings(vect, params['max_num_words'], params['max_seq_length'], name=dataset_name, embedding_dim=params['embedding_dim'], filename=vectors_filename, type=embedding_type)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
