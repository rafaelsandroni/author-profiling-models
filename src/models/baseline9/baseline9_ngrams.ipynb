{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Activation, Input, Dense, Flatten, Dropout, Embedding\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import regularizers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "result = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.functions.plot import plot_history, full_multiclass_report\n",
    "from Models.functions.preprocessing import clean, labelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.layers import Convolution1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import ThresholdedReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CharCNNZhang\n",
    "def build_model( \n",
    "                input_size, \n",
    "                alphabet_size, \n",
    "                conv_layers,\n",
    "                fully_connected_layers,\n",
    "                embedding_size, \n",
    "                threshold, \n",
    "                dropout_p, \n",
    "                num_of_classes, \n",
    "                optimizer='adam', \n",
    "                #loss='categorical_crossentropy'\n",
    "                loss='sparse_categorical_crossentropy'\n",
    "               ):\n",
    "    \"\"\"\n",
    "    Build and compile the Character Level CNN model\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    inputs = Input(shape=(input_size,), name='sent_input', dtype='int64')\n",
    "    # Embedding layers\n",
    "    x = Embedding(alphabet_size + 1, embedding_size, input_length=input_size)(inputs)\n",
    "    # Convolution layers\n",
    "    for cl in conv_layers:\n",
    "        x = Convolution1D(cl[0], cl[1])(x)\n",
    "        x = ThresholdedReLU(threshold)(x)\n",
    "        if cl[2] != -1:\n",
    "            x = MaxPooling1D(cl[2])(x)\n",
    "    x = Flatten()(x)\n",
    "    # Fully connected layers\n",
    "    for fl in fully_connected_layers:\n",
    "        x = Dense(fl)(x)\n",
    "        x = ThresholdedReLU(threshold)(x)\n",
    "        x = Dropout(dropout_p)(x)\n",
    "    # Output layer\n",
    "    predictions = Dense(num_of_classes, activation='softmax')(x)\n",
    "    # Build and compile model\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    model = model\n",
    "    print(\"CharCNNZhang model built: \")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from Models.functions.datasets import loadTrainTest\n",
    "\n",
    "X, _, y, _ = loadTrainTest(\"gender\", \"brblogset\", \"/home/rafael/GDrive/Data/Dataframe/\")\n",
    "y, n_classes, classes_names = labelEncoder(y)\n",
    "#vect = CountVectorizer(analyzer=\"word\", max_features=30000)\n",
    "#X_tfidf = vect.fit_transform(X).toarray()\n",
    "#X_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38567, 16186)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = int(np.mean([len(i) for i in X.values]))\n",
    "median = int(np.median([len(i) for i in X.values]))\n",
    "mean, median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(data):\n",
    "\n",
    "    # str_to_indexes\n",
    "    def str_to_indexes(s, length, dict_char):\n",
    "        \"\"\"\n",
    "        Convert a string to character indexes based on character dictionary.\n",
    "\n",
    "        Args:\n",
    "            s (str): String to be converted to indexes\n",
    "        Returns:\n",
    "            str2idx (np.ndarray): Indexes of characters in s\n",
    "        \"\"\"\n",
    "        s = s.lower()\n",
    "        max_length = min(len(s), length)\n",
    "        str2idx = np.zeros(length, dtype='int64')\n",
    "        for i in range(1, max_length + 1):\n",
    "            c = s[-i]\n",
    "            if c in dict_char:\n",
    "                str2idx[i - 1] = dict_char[c]\n",
    "        return str2idx\n",
    "        \n",
    "\n",
    "    alphabet=\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "    alphabet_size = len(alphabet)\n",
    "    input_size = 1024\n",
    "    dict_char = {}\n",
    "    for idx, char in enumerate(alphabet):\n",
    "        dict_char[char] = idx + 1\n",
    "    length = input_size\n",
    "\n",
    "    data_size = len(data)\n",
    "    \n",
    "    \"\"\"\n",
    "    Return all loaded data from data variable.\n",
    "    Returns:\n",
    "        (np.ndarray) Data transformed from raw to indexed form with associated one-hot label.\n",
    "    \"\"\"\n",
    "    data_size = len(data)\n",
    "    start_index = 0\n",
    "    end_index = data_size\n",
    "    batch_texts = data[start_index:end_index]\n",
    "    batch_indices = []\n",
    "    #one_hot = np.eye(no_of_classes, dtype='int64')\n",
    "    classes = []\n",
    "    for s in batch_texts:\n",
    "        batch_indices.append(str_to_indexes(s, length, dict_char))\n",
    "        #c = int(c) - 1\n",
    "        #classes.append(one_hot[c])\n",
    "    return np.asarray(batch_indices, dtype='int64'), np.asarray(classes), dict_char, alphabet_size\n",
    "\n",
    "X_char, cl, dict_char, alphabet_size = format_data(X.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((417, 1024), (1024,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_char, y, test_size = 0.2)\n",
    "\n",
    "#X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "#X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "#y_train = to_categorical(y_train, 2)\n",
    "#y_test = to_categorical(y_test, 2)\n",
    "\n",
    "x_test.shape, x_train[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-30db9dd1bddd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = dict(\n",
    "        input_size = X_train.shape[1],\n",
    "        alphabet_size = alphabet_size,\n",
    "        embedding_size = 128,\n",
    "        # feature maps, kernel, maxpooling\n",
    "        conv_layers = [[256,7,3]],#,[256,7,3],[256,3,-1],[256,3,-1],[256,3,-1],[256,3,3]],\n",
    "        fully_connected_layers = [1024],#, 2014],\n",
    "        threshold = 1e-6,\n",
    "        dropout_p = 0.5,\n",
    "        num_of_classes = n_classes,\n",
    "        epochs = 5000,\n",
    "        batch_size = 64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = params_grid\n",
    "\n",
    "## create the model with the best params found\n",
    "model = build_model(\n",
    "    input_size=params[\"input_size\"],\n",
    "     alphabet_size=params[\"alphabet_size\"],\n",
    "     embedding_size=params[\"embedding_size\"],\n",
    "     conv_layers=params[\"conv_layers\"],\n",
    "     fully_connected_layers=params[\"fully_connected_layers\"],\n",
    "     num_of_classes=params[\"num_of_classes\"],\n",
    "     threshold=params[\"threshold\"],\n",
    "     dropout_p=params[\"dropout_p\"],\n",
    "     #optimizer=params[\"optimizer\"],\n",
    "     #loss=params[\"loss\"])\n",
    ")\n",
    "## Then train it and display the results\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=params['epochs'],\n",
    "                    validation_split=0.2,\n",
    "                    batch_size=params['batch_size'],\n",
    "                    verbose = 1,\n",
    "                       callbacks=[\n",
    "                           #ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, min_lr=0.01),\n",
    "                           EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "                  ])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "directory='/home/rafael/'\n",
    "\n",
    "plot_history(history, directory=directory, show=True)\n",
    "\n",
    "full_multiclass_report(model,\n",
    "                       x_test,\n",
    "                       y_test,\n",
    "                       classes=classes_names,\n",
    "                       directory=directory\n",
    "                      )\n",
    "                       #batch_size=32,\n",
    "                       #binary= )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
