{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from Models.functions.plot import plot_history, full_multiclass_report, plot_confusion_matrix\n",
    "from Models.functions.preprocessing import clean, labelEncoder\n",
    "from Models.functions.datasets import loadTrainTest\n",
    "from Models.functions.utils import checkFolder, listProblems\n",
    "from Models.functions.transform import tokenizer_pad_sequence\n",
    "from Models.functions.vectors import create_embeddings, train_vectors\n",
    "from Models.functions.cnn_model import build_cnn1\n",
    "\n",
    "\n",
    "from keras.layers import Activation, Input, Dense, Flatten, Dropout, Embedding\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import regularizers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "import collections, numpy\n",
    "import gc\n",
    "from time import time, sleep\n",
    "\n",
    "results_dataframe = \"/reports_grid/results.csv\"\n",
    "try:\n",
    "    results = pd.read_csv(results_dataframe)\n",
    "except:\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model(filters = [10], kernel_size = [5], strides = [100], \n",
    "                 dropout_rate = 0.5, pool_size = [5], dense_units = 512, max_len = 1000, n_classes = 2, optimizer = 'rmsprop'):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # conv 1\n",
    "    model.add(Conv1D(filters = filters[0], \n",
    "                     kernel_size = kernel_size[0],\n",
    "                     strides = strides[0], \n",
    "                     activation = 'relu', \n",
    "                     input_shape = (max_len, 100)))\n",
    "                     #activity_regularizer = regularizers.l2(0.2)))\n",
    "\n",
    "    # pooling layer 1\n",
    "    \n",
    "    model.add(MaxPooling1D(pool_size = pool_size[0], strides = 1))\n",
    "    model.add(Activation('relu'))\n",
    "    \"\"\"\n",
    "    model.add(Conv1D(filters = filters[1], \n",
    "                     kernel_size = kernel_size[1],\n",
    "                     strides = strides[0], \n",
    "                     activation = 'relu',\n",
    "                     activity_regularizer = regularizers.l2(0.2)))\n",
    "    \n",
    "    model.add(MaxPooling1D(pool_size = pool_size[1], strides = 1))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv1D(filters = filters[2], \n",
    "                     kernel_size = kernel_size[2],\n",
    "                     strides = strides[0], \n",
    "                     activation = 'relu',\n",
    "                     activity_regularizer = regularizers.l2(0.2)))\n",
    "    \n",
    "    model.add(MaxPooling1D(pool_size = pool_size[2], strides = 1))\n",
    "    model.add(Activation('relu'))\n",
    "    \"\"\"\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    if dropout_rate is not None:\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        \n",
    "    model.add(Dense(units = dense_units, activation = 'relu'))\n",
    "    model.add(Dense(units = n_classes, activation = 'softmax'))\n",
    "\n",
    "    #TODO: test others foss functions: https://keras.io/losses/\n",
    "    model.compile(optimizer = optimizer, loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def garbage_collection(): \n",
    "    gc.collect()\n",
    "    print(\"gargabe colletion...\")\n",
    "    sleep(3)\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "task = \"gender\"\n",
    "dataset_name = \"brmoral\"\n",
    "lang = \"pt\"\n",
    "root = \"/home/rafael/Dataframe/\"\n",
    "\n",
    "# Synthetic Minority Oversampling Technique (SMOTE)\n",
    "def oversampling(X, y):\n",
    "    try:\n",
    "        X_resampled, y_resampled = SMOTE().fit_resample(X, y)\n",
    "    except:\n",
    "        X_resampled, y_resampled = X, y\n",
    "        \n",
    "    return X_resampled, y_resampled\n",
    "    # return X, y\n",
    "\n",
    "def train_val_metrics(histories):\n",
    "    print('Training: \\t%0.4f loss / %0.4f acc' % (get_avg(histories, 'loss'), get_avg(histories, 'acc')))\n",
    "    print('Validation: \\t%0.4f loss / %0.4f acc' % (get_avg(histories, 'val_loss'), get_avg(histories, 'val_acc')))\n",
    "\n",
    "def get_avg(histories, his_key):\n",
    "    tmp = []\n",
    "    for history in histories:\n",
    "        tmp.append(history[his_key][np.argmin(history['val_loss'])])\n",
    "    return np.mean(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################\n",
      " RUNNING 1 PROBLEMS\n",
      " Dataset:  brblogset  / Task: education  / Lang: pt\n",
      "education brblogset /home/rafael/Dataframe/ pt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "run_all = True\n",
    "\n",
    "g_root              = '/home/rafael/Dataframe/'\n",
    "\n",
    "filter_dataset_name = 'brblogset'\n",
    "\n",
    "filter_task         = 'education'\n",
    "\n",
    "report_version = 'Reports_emb'\n",
    "\n",
    "if run_all == True:\n",
    "    args = []\n",
    "    problems = listProblems(filter_dataset_name, filter_task)\n",
    "    print(\"############################################\")\n",
    "    print(\" RUNNING {0} PROBLEMS\".format(len(problems)))\n",
    "\n",
    "    # create a list of tasks\n",
    "    for task, dataset_name, lang in problems:\n",
    "\n",
    "        #args.append([task, dataset_name, g_root, lang])\n",
    "        print(\" Dataset: \",dataset_name,\" / Task:\",task,\" / Lang:\",lang)\n",
    "        #run(task, dataset_name, g_root, lang)    \n",
    "print(task, dataset_name, g_root, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max:  142632  / mean:  3224.6881463802706  / median:  1595.0\n"
     ]
    }
   ],
   "source": [
    "#def run(task, dataset_name, root, lang, report_version = 'Reports'):\n",
    "\n",
    "histories = []\n",
    "test_loss = []\n",
    "test_accs = []\n",
    "\n",
    "predicted_y = []\n",
    "predicted_y_proba = []\n",
    "expected_y = []\n",
    "\n",
    "directory='./'+ report_version +'/'+task+'/'+dataset_name+'_'+lang+'/'\n",
    "\n",
    "checkFolder(directory)\n",
    "\n",
    "X, _, y, _ = loadTrainTest(task, dataset_name, root, lang)\n",
    "\n",
    "X = X.apply(clean, lang=lang)\n",
    "X = X.values # mandatory for pan13\n",
    "\n",
    "y, n_classes, classes_names = labelEncoder(y)    \n",
    "\n",
    "max_length = np.max([len(x.split(\" \")) for x in X])\n",
    "mean_length = np.mean([len(x.split(\" \")) for x in X])\n",
    "median_length = np.median([len(x.split(\" \")) for x in X])\n",
    "\n",
    "print(\"max: \", max_length, \" / mean: \", mean_length, \" / median: \", median_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    filters = [50, 50, 50],\n",
    "    kernel_size = [3],\n",
    "    strides = [1, 1, 1],\n",
    "    dropout_rate = 0.15,\n",
    "    pool_size = [2],\n",
    "    epochs = 100,\n",
    "    batch_size = 12,\n",
    "    embedding_dim = 100,\n",
    "    dense_units = [512],\n",
    "    max_num_words = None,\n",
    "    max_seq_length = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "text = X\n",
    "length = [len(x) for x in text]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=None, char_level=True)\n",
    "\n",
    "tokenizer.fit_on_texts(text)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# MAX_SEQ_LENGTH = np.max(arr_length)\n",
    "params['max_seq_length'] = int(np.mean(length))\n",
    "\n",
    "\n",
    "# Padding all sequences to same length of `max_seq_length`\n",
    "X2 = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(3,3), analyzer='char')\n",
    "tfidf.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  del sys.path[0]\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 929605 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#vect.index_word[648], \n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "#vectorName = r'/home/rafael/GDrive/Embeddings/fasttext/'+ dataset_name +'_sg_'+ str(params['embedding_dim']) +'dim.model'        \n",
    "vectorName = r'/home/rafael/GDrive/Embeddings/nilc/fasttext_pt_skip_s100.txt'\n",
    "embedding_type = 1\n",
    "#model_ug_sg = Word2Vec.load(vectorName)\n",
    "model_ug_sg = KeyedVectors.load_word2vec_format(vectorName, binary=False, unicode_errors=\"ignore\")\n",
    "\n",
    "embeddings_index = {}\n",
    "for w in model_ug_sg.wv.vocab.keys():\n",
    "    #embeddings_index[w] = np.append(model_ug_cbow.wv[w],model_ug_sg.wv[w])\n",
    "    embeddings_index[w] = model_ug_sg.wv[w]\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num words 3224 / emb_dim 100\n",
      "new_X (1257, 3224, 100)\n"
     ]
    }
   ],
   "source": [
    "num_words = int(mean_length)\n",
    "embedding_dim = 100\n",
    "print(\"num words {0} / emb_dim {1}\".format(num_words, embedding_dim))\n",
    "\n",
    "new_X = []\n",
    "for texts in X:        \n",
    "    instance = np.zeros((num_words, embedding_dim))\n",
    "    i = 0\n",
    "    for word in texts.split(\" \"):\n",
    "        new_word = []\n",
    "        for w in word:\n",
    "            if w in 'abcdefghijklmnopqrstuvxyz':\n",
    "                new_word.append(w)\n",
    "        word = \"\".join(new_word)\n",
    "        \n",
    "        \n",
    "        if i >= num_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word.lower())\n",
    "        if embedding_vector is not None:            \n",
    "            instance[i] = embedding_vector\n",
    "        else:\n",
    "            instance[i] = np.zeros(embedding_dim)\n",
    "            \n",
    "        i += 1\n",
    "    new_X.append(instance)\n",
    "\n",
    "new_X = np.array(new_X)\n",
    "\n",
    "print(\"new_X\", new_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeros 2251884 / total 4052568\n",
      "0.555668405810834\n"
     ]
    }
   ],
   "source": [
    "# words not found\n",
    "count_zeros = lambda x: 1 if sum(x) == 0 else 0\n",
    "c2 = 0\n",
    "c1 = 0\n",
    "for x in new_X:\n",
    "    s = sum(list(map(count_zeros, x)))\n",
    "    c1 += s\n",
    "    c2 += len(x)\n",
    "    \n",
    "print(\"zeros {0} / total {1}\".format(c1, c2))\n",
    "print( c1 / c2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = []\n",
    "m = 0\n",
    "for x in X:\n",
    "    i = []\n",
    "    for index in range(3, len(x), 1):\n",
    "        ch = str(x[index-3:index])\n",
    "        if ch in tfidf.vocabulary_:\n",
    "            index = tfidf.vocabulary_[ch]\n",
    "            try:\n",
    "                emb = embedding_matrix[index]\n",
    "            except:\n",
    "                emb = np.zeros(100)\n",
    "            idf = 1            \n",
    "            emb = emb * idf\n",
    "        else:\n",
    "            m += 1\n",
    "            emb = np.zeros(100)\n",
    "            \n",
    "        i.append(emb)\n",
    "        pass\n",
    "    X1.append(np.mean(i, axis=1))\n",
    "    pass\n",
    "X1 = np.array(X1)\n",
    "\n",
    "\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = to_categorical(y, n_classes)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_X, y1, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1005, 3224, 100), (1005, 4), (252, 3224, 100), 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3222, 50)          15050     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 3221, 50)          0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 3221, 50)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 161050)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 161050)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               82458112  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 82,475,214\n",
      "Trainable params: 82,475,214\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "model = create_model(\n",
    "                        max_len=X_train.shape[1],\n",
    "                        n_classes=n_classes,\n",
    "                        filters=params['filters'],\n",
    "                        kernel_size=params['kernel_size'],\n",
    "                        strides=params['strides'],                        \n",
    "                        dropout_rate=params['dropout_rate'],\n",
    "                        pool_size=params['pool_size']\n",
    "                    )\n",
    "\"\"\"\n",
    "model = build_cnn1(\n",
    "                embedding_layer=embedding_layer,\n",
    "                num_words=params['max_num_words'],\n",
    "                embedding_dim=params['embedding_dim'],\n",
    "                filter_sizes=[3],\n",
    "                feature_maps=[50],\n",
    "                max_seq_length=100,\n",
    "                dropout_rate=params['dropout_rate'] or None,\n",
    "                dense_units=params['dense_units'] or 512,\n",
    "                n_classes=n_classes,\n",
    "                pool_size=[2,2],\n",
    "                strides=[1,2]\n",
    "        )\n",
    "\"\"\"\n",
    "\n",
    "model.compile(\n",
    "                loss='categorical_crossentropy',\n",
    "                #loss='mean_squared_error',\n",
    "                optimizer='rmsprop',\n",
    "                metrics=['accuracy','mae']\n",
    "        )\n",
    "\n",
    "model.summary()\n",
    "        \n",
    "## Then train it and display the results\n",
    "history = model.fit(X_train,\n",
    "                    y_train,                            \n",
    "                    validation_split=0.2,   \n",
    "                    verbose = 0,\n",
    "                    batch_size=params['batch_size'],                                \n",
    "                    epochs=500,\n",
    "                    callbacks=[\n",
    "                        #ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, min_lr=0.01),\n",
    "                        EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=0)\n",
    "                ])        \n",
    "\n",
    "y_pred_proba = model.predict(X_test, batch_size=params['batch_size'])\n",
    "predicted_y_proba.extend(y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy : 0.3412698412698413\n",
      "F1-Score : 0.3016862804131136\n",
      "\n",
      "Classification Report\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "             Básico      0.450     0.188     0.265        48\n",
      "       Pós-graduado      0.369     0.488     0.421        84\n",
      "  Superior Completo      0.312     0.417     0.357        72\n",
      "Superior Incompleto      0.240     0.125     0.164        48\n",
      "\n",
      "          micro avg      0.341     0.341     0.341       252\n",
      "          macro avg      0.343     0.304     0.302       252\n",
      "       weighted avg      0.344     0.341     0.324       252\n",
      "\n",
      "[[ 9 18 17  4]\n",
      " [ 5 41 30  8]\n",
      " [ 6 29 30  7]\n",
      " [ 0 23 19  6]]\n",
      "Normalized confusion matrix\n",
      "[[0.1875     0.375      0.35416667 0.08333333]\n",
      " [0.05952381 0.48809524 0.35714286 0.0952381 ]\n",
      " [0.08333333 0.40277778 0.41666667 0.09722222]\n",
      " [0.         0.47916667 0.39583333 0.125     ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9b7c69bcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print()\n",
    "y_test = np.argmax(y_test,axis=1)\n",
    "y_pred = y_pred_proba.argmax(axis=1)\n",
    "\n",
    "# 3. Print accuracy score\n",
    "print(\"Accuracy : \"+ str(accuracy_score(y_test,y_pred)))    \n",
    "print(\"F1-Score : \"+ str(f1_score(y_test,y_pred,average=\"macro\")))\n",
    "print(\"\")\n",
    "\n",
    "# 4. Print classification report\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test,y_pred, digits=3, target_names=classes_names, output_dict=False))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y_test,y_pred), classes=classes_names, directory='/tmp/', normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "K = StratifiedKFold(n_splits=3)\n",
    "idx = 0\n",
    "for train_index, test_index in K.split(X, y):\n",
    "\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2) \n",
    "    vect = None\n",
    "    vect = TfidfVectorizer(max_features=None)        \n",
    "    X_train = vect.fit_transform(X_train).toarray()\n",
    "    X_test = vect.transform(X_test).toarray()\n",
    "\n",
    "    X_train, y_train = oversampling(X_train, y_train)\n",
    "    X_test,  y_test  = oversampling(X_test, y_test)\n",
    "\n",
    "    y_train = to_categorical(y_train, n_classes)\n",
    "    y_test  = to_categorical(y_test, n_classes)\n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    # validation\n",
    "    validation_split = 0.1\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = validation_split) \n",
    "\n",
    "\n",
    "\n",
    "    print(\"TFIDF\")      \n",
    "\n",
    "    print(\"Word embedding\")\n",
    "\n",
    "    _, _, _, vect  = tokenizer_pad_sequence(X[train_index], params['max_num_words'],  params['max_seq_length'])    \n",
    "\n",
    "    vectors_filename = r'/home/rafael/GDrive/Embeddings/word2vec/'+ dataset_name +'_sg_'+ str(params['embedding_dim']) +'dim.model'        \n",
    "    embedding_type = 1\n",
    "    embedding_matrix = create_embeddings(vect, params['max_num_words'], params['max_seq_length'], name=dataset_name, embedding_dim=params['embedding_dim'], filename=vectors_filename, type=embedding_type, return_matrix=True)        \n",
    "    print(embedding_matrix.shape)\n",
    "\n",
    "\n",
    "    ## create the model with the best params found\n",
    "    #model = KerasClassifier(build_fn=create_model,\n",
    "    model = None\n",
    "    model = create_model(\n",
    "                            max_len=X_train.shape[1],\n",
    "                            n_classes=n_classes,\n",
    "                            filters=params['filters'],\n",
    "                            kernel_size=params['kernel_size'],\n",
    "                            strides=params['strides'],                        \n",
    "                            dropout_rate=params['dropout_rate'],\n",
    "                            pool_size=params['pool_size']\n",
    "                        )\n",
    "\n",
    "    ## Then train it and display the results\n",
    "    history = model.fit(X_train,\n",
    "                        y_train,                            \n",
    "                        validation_data=(X_val, y_val),                            \n",
    "                        verbose = 1,\n",
    "                        batch_size=params['batch_size'],                                \n",
    "                        epochs=params['epochs'],\n",
    "                        callbacks=[\n",
    "                            #ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, min_lr=0.01),\n",
    "                            EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=0)\n",
    "                    ])        \n",
    "\n",
    "    y_pred_proba = model.predict(X_test, batch_size=params['batch_size'])\n",
    "    predicted_y_proba.extend(y_pred_proba)\n",
    "\n",
    "    binary = False # True if len(classes) < 3 else False\n",
    "    # 1. Transform one-hot encoded y_test into their class number\n",
    "    if not binary:\n",
    "        y_test = np.argmax(y_test,axis=1)\n",
    "\n",
    "    # 2. Predict classes and stores \n",
    "    #y_pred = model.predict(X_test, batch_size=params['batch_size'])        \n",
    "    y_pred = y_pred_proba.argmax(axis=1)\n",
    "\n",
    "    predicted_y.extend(y_pred)\n",
    "    expected_y.extend(y_test)\n",
    "    histories.append(history.history)\n",
    "    garbage_collection()\n",
    "\n",
    "del X, y, model, vect\n",
    "\n",
    "expected_y = np.array(expected_y)\n",
    "predicted_y = np.array(predicted_y)\n",
    "predicted_y_proba = np.array(predicted_y)\n",
    "\n",
    "np.save(directory + '/expected.numpy', expected_y)\n",
    "np.save(directory + '/predicted.numpy', predicted_y)\n",
    "np.save(directory + '/predicted_proba.numpy', predicted_y_proba)\n",
    "with open(directory + '/histories.pkl', 'wb') as f:\n",
    "    pickle.dump(histories, f)\n",
    "\n",
    "# metrics    \n",
    "train_val_metrics(histories)\n",
    "\n",
    "# plot_history(histories, directory)\n",
    "\n",
    "# y_pred = model.predict(x, batch_size=batch_size)\n",
    "\n",
    "# 3. Print accuracy score\n",
    "print(\"Accuracy : \"+ str(accuracy_score(expected_y,predicted_y)))    \n",
    "print(\"F1-Score : \"+ str(f1_score(expected_y,predicted_y,average=\"macro\")))    \n",
    "print(\"\")\n",
    "\n",
    "# 4. Print classification report\n",
    "print(\"Classification Report\")\n",
    "report = pd.DataFrame(\n",
    "    classification_report(expected_y, predicted_y, digits=3, target_names=classes_names, output_dict=True)\n",
    ")\n",
    "report = report.transpose()\n",
    "accuracy = accuracy_score(expected_y, predicted_y)\n",
    "report['accuracy'] = [accuracy] * (n_classes + 3)    \n",
    "report.to_csv(directory + '/report.csv')\n",
    "print(report)\n",
    "\n",
    "# 5. Plot confusion matrix\n",
    "cnf_matrix = confusion_matrix(expected_y,predicted_y)    \n",
    "np.save(directory + \"/confusion_matrix\", np.array(cnf_matrix))    \n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_names, directory=directory, normalize=True)\n",
    "\n",
    "# 6. Clean memory\n",
    "garbage_collection()\n",
    "gc.collect()\n",
    "\n",
    "print(\"+\"+\"-\"*50+\"+\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
