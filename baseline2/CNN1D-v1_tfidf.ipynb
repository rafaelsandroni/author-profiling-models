{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (plot.py, line 275)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2961\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-2-2e3ab0cf75b1>\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from Models.functions.plot import plot_history, full_multiclass_report\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m969\u001b[0m, in \u001b[1;35m_find_and_load\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m954\u001b[0m, in \u001b[1;35m_find_and_load_unlocked\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m896\u001b[0m, in \u001b[1;35m_find_spec\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap_external>\"\u001b[0m, line \u001b[1;32m1139\u001b[0m, in \u001b[1;35mfind_spec\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap_external>\"\u001b[0m, line \u001b[1;32m1115\u001b[0m, in \u001b[1;35m_get_spec\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap_external>\"\u001b[0m, line \u001b[1;32m1096\u001b[0m, in \u001b[1;35m_legacy_get_spec\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m444\u001b[0m, in \u001b[1;35mspec_from_loader\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<frozen importlib._bootstrap_external>\"\u001b[0;36m, line \u001b[0;32m533\u001b[0;36m, in \u001b[0;35mspec_from_file_location\u001b[0;36m\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/usr/local/lib/python3.5/dist-packages/Models-1.0-py3.5.egg/Models/functions/plot.py\"\u001b[0;36m, line \u001b[0;32m275\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "from Models.functions.plot import plot_history, full_multiclass_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Input, Dense, Flatten, Dropout, Embedding\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import regularizers\n",
    "from keras.models import Model, Sequential\n",
    "#from Models.functions.preprocessing import clean\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(filters = [100], kernel_size = [50], strides = [100], \n",
    "                 dropout_rate = [0.5], pool_size = [5], max_len = 1000):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # conv 1\n",
    "    model.add(Conv1D(filters = filters[0], \n",
    "                     kernel_size = kernel_size[0],\n",
    "                     strides = strides[0], \n",
    "                     activation = 'relu', \n",
    "                     input_shape = (max_len, 1) ))\n",
    "\n",
    "    # pooling layer 1\n",
    "    model.add(MaxPooling1D(pool_size = pool_size[0], strides = 1))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units = 100, activation = 'relu'))\n",
    "    model.add(Dense(units = 2, activation = 'softmax'))\n",
    "\n",
    "    model.compile(optimizer = 'adadelta', loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result = pd.DataFrame()\n",
    "\n",
    "def get_results(model):\n",
    "\n",
    "    config = model.get_config()\n",
    "\n",
    "    row = {}\n",
    "\n",
    "    conv_layers = np.sum([1 if i['class_name'] == \"Conv1D\" else 0 for i in config])\n",
    "    pooling_layers = np.sum([1 if i['class_name'] == \"MaxPooling1D\" else 0 for i in config])\n",
    "\n",
    "    row.update({ '_accuracy': accuracy_score(y_espected, y_predicted) })\n",
    "    row.update({ '_f1-score': f1_score(y_espected, y_predicted,average='weighted')})\n",
    "    row.update({ 'conv_layers': conv_layers })\n",
    "    row.update({ 'pooling_layers': pooling_layers })\n",
    "\n",
    "    _, _, fscore, support = precision_recall_fscore_support(y_espected, y_predicted)\n",
    "\n",
    "    [row.update({'_fscore_class_'+str(i[0]): i[1]}) for i in enumerate(fscore)]\n",
    "    [row.update({'_support_class_'+str(i[0]): i[1]}) for i in enumerate(support)]\n",
    "\n",
    "    idx = 1\n",
    "    for i in config:\n",
    "        if i['class_name'] == \"Conv1D\":\n",
    "            j = str(idx)\n",
    "            row.update({\n",
    "                'filters_'+j: i['config']['filters'],\n",
    "                'strides_'+j: i['config']['strides'],\n",
    "                'kernel_size_'+j: i['config']['kernel_size'],\n",
    "                'activation_'+j: i['config']['activation']\n",
    "            })\n",
    "        pass\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelEncoder(y):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(y)\n",
    "    return (le.transform(y), len(le.classes_), list(le.classes_))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.functions.datasets import loadTrainTest\n",
    "X_train, X_test, y_train, y_test = loadTrainTest(\"gender\", \"b5post\", \"/home/rafael/GDrive/Data/Dataframe/\")\n",
    "\n",
    "y_train, n, _ = labelEncoder(y_train)\n",
    "y_test, n, classes_names = labelEncoder(y_test)\n",
    "\n",
    "X = np.concatenate([X_train, X_test])\n",
    "\n",
    "vect = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "vect.fit(X)\n",
    "\n",
    "X.shape, X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = vect.transform(X_train).toarray()\n",
    "X_test_tfidf = vect.transform(X_test).toarray()\n",
    "\n",
    "X_train_tfidf.shape, X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = np.max([len(x.split(\" \")) for x in X_train])\n",
    "mean_length = np.mean([len(x.split(\" \")) for x in X_train])\n",
    "mediam_length = np.median([len(x.split(\" \")) for x in X_train])\n",
    "\n",
    "print(max_length, mean_length, mediam_length, int(mediam_length), int(mediam_length) == 2255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import collections, numpy\n",
    "\n",
    "# Synthetic Minority Oversampling Technique (SMOTE)\n",
    "def oversampling(X, y):\n",
    "    X_resampled, y_resampled = SMOTE().fit_resample(X, y)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "X_resampled, y_resampled = oversampling(X_train_tfidf, y_train)\n",
    "\n",
    "#scaler = StandardScaler().fit(X)\n",
    "#m = scaler.transform(X)\n",
    "#m = matrix2\n",
    "\n",
    "collections.Counter(y_train), collections.Counter(y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size = 0.2)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "#y_train = to_categorical(y_train, 2)\n",
    "#y_test = to_categorical(y_test, 2)\n",
    "print(len(y_train), len(x_train))\n",
    "print(len(y_test), len(x_test))\n",
    "#print()\n",
    "#print(y_test)\n",
    "\n",
    "x_test.shape, x_train[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xid, yid = 10, 222\n",
    "plt.scatter(x_train[:,xid], x_train[:,yid],c=y_train)\n",
    "#plt.ylabel(classes_names[0])\n",
    "#plt.ylabel(classes_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "CNN = KerasClassifier(build_fn=create_model, \n",
    "                epochs=1,\n",
    "                #max_len=x_train[1],\n",
    "                #batch_size=BATCH_SIZE,\n",
    "                verbose=0,\n",
    "                validation_split=0.1\n",
    "                )\n",
    "\n",
    "params_grid = dict(\n",
    "        filters = [[40]],\n",
    "        kernel_size = [[50]],\n",
    "        strides = [[100]],\n",
    "        dropout_rate = [[0.5]],\n",
    "        pool_size = [[5]],\n",
    "        epochs = [1]        \n",
    "        #batch_size = 100\n",
    ")                               \n",
    "\n",
    "grid_search = GridSearchCV(CNN, \n",
    "                           params_grid, \n",
    "                           scoring='accuracy', cv=3, \n",
    "                           return_train_score=True\n",
    "                           )\n",
    "\n",
    "#print('best params', grid_search.best_params_)\n",
    "\n",
    "\n",
    "print(\"Performing grid search...\")    \n",
    "t0 = time()\n",
    "\n",
    "#histories = []\n",
    "grid_results = grid_search.fit(x_train,y_train)\n",
    "\n",
    "print(\"done in %0.2fs and %0.1fmin\" % ((time() - t0), ((time() - t0) / 60) ))\n",
    "print()\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(params_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "display(pd.DataFrame(grid_search.cv_results_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params = best_parameters\n",
    "\n",
    "## create the model with the best params found\n",
    "model = create_model(filters=params['filters'],\n",
    "                     kernel_size=params['kernel_size'],\n",
    "                     strides=params['strides'],\n",
    "                     dropout_rate=params['dropout_rate'],\n",
    "                     pool_size=params['pool_size']\n",
    "                    )\n",
    "\n",
    "## Then train it and display the results\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=5,#params['epochs'],\n",
    "                    #batch_size=params['batch_size'],\n",
    "                    verbose = 0)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "plot_history(history)\n",
    "\n",
    "\n",
    "full_multiclass_report(model,\n",
    "                       x_test,\n",
    "                       y_test,\n",
    "                       classes=classes_names\n",
    "                      )\n",
    "                       #batch_size=32,\n",
    "                       #binary= )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.functions import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
