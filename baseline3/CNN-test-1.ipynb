{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from Models.functions.plot import ROC, plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.functions.datasets import getDatasets\n",
    "\n",
    "from Models.functions.preprocessing import clean\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import re, string\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "#from Models.functions.plot import ROC, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def labelEncoder(y):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y)\n",
    "\n",
    "    return (le.transform(y), len(le.classes_), list(le.classes_))\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import re, string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'age'\n",
    "dataset_name = 'brblogset'\n",
    "MAX_FEATURES = 5000\n",
    "\n",
    "datasets = getDatasets(task,'df', dataset_name)\n",
    "for i in datasets.iterrows():\n",
    "\n",
    "    name = i[1]['dataset_name']\n",
    "    label = task\n",
    "    ds_path = i[1]['path']\n",
    "\n",
    "    # load training and test dataframes\n",
    "    training_path = ds_path + '/' + i[1]['training']        \n",
    "    #test_path = ds_path + '/' + i[1]['test']      \n",
    "\n",
    "    df_training = pd.read_csv(training_path)#, usecols=cols)        \n",
    "    #df_test = pd.read_csv(test_path)#, usecols=cols)        \n",
    "\n",
    "    df_training['text'] = df_training['text'].apply(clean)\n",
    "    #df_test['text'] = df_test['text'].apply(clean)\n",
    "    X = df_training['text'].values\n",
    "    y, n_classes, classes_name = labelEncoder(df_training[label].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2081"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(lines):\n",
    "    \"\"\"\n",
    "    Calculate the maximum document length\n",
    "    \"\"\"\n",
    "    return max([len(s.split()) for s in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text informations:\n",
      "max length: 246111 / min length: 0 / mean length: 2837 \n"
     ]
    }
   ],
   "source": [
    "max_length = max_length(X)\n",
    "\n",
    "result = [len(x.split()) for x in X]\n",
    "print('Text informations:')\n",
    "print('max length: %i / min length: %i / mean length: %i ' % (np.max(result),\n",
    "                                                                np.min(result),\n",
    "                                                                np.mean(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    \"\"\"\n",
    "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
    "    \n",
    "    get_top_n_words([\"I love Python\", \"Python is a language programming\", \"Hello world\", \"I love the world\"]) -> \n",
    "    [('python', 2),\n",
    "     ('world', 2),\n",
    "     ('love', 2),\n",
    "     ('hello', 1),\n",
    "     ('is', 1),\n",
    "     ('programming', 1),\n",
    "     ('the', 1),\n",
    "     ('language', 1)]\n",
    "    \"\"\"\n",
    "    vec = TfidfVectorizer(max_features=MAX_FEATURES).fit(corpus)\n",
    "    # vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "a10-25    526\n",
      "a26-40    817\n",
      "a40+      738\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_training.groupby(task).size())\n",
    "c = df_training.groupby(task).size().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a10-25\n",
      "pra 19.11713110329236\n",
      "ser 18.26320598815446\n",
      "dia 13.912443381687353\n",
      "tudo 13.886424874445177\n",
      "vida 13.751203620880371\n",
      "bem 11.618026516751847\n",
      "aqui 10.90112017452841\n",
      "ainda 10.60086446021793\n",
      "sobre 10.512026346315631\n",
      "amor 10.446542626423561\n",
      "--\n",
      "a26-40\n",
      "ser 26.773788014688567\n",
      "dia 21.242673043981682\n",
      "vida 18.896776750077915\n",
      "sobre 17.20846259592052\n",
      "pra 16.9670529032183\n",
      "bem 16.385928137988184\n",
      "se 16.381608526791\n",
      "deus 16.28922200731576\n",
      "tudo 15.435237009843586\n",
      "ainda 14.985437523534223\n",
      "--\n",
      "a40+\n",
      "ser 25.053980419007935\n",
      "se 19.15082968186628\n",
      "vida 18.809873674024143\n",
      "dia 18.46133564329977\n",
      "sobre 16.960065818193332\n",
      "deus 16.936701915912735\n",
      "anos 16.059508678059544\n",
      "todos 14.901210320329193\n",
      "tudo 14.060458791293323\n",
      "the 13.60200562041761\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_classes):\n",
    "    c1 = df_training[df_training[task] == c[i]]\n",
    "    print(classes_name[i])\n",
    "    text = [line.replace(\"\\n\", \"\") for line in c1.text]\n",
    "    common_words = get_top_n_words(text, 10)\n",
    "    for word, freq in common_words:\n",
    "        print(word, freq)\n",
    "    print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "vec = TfidfVectorizer(max_features=MAX_FEATURES).fit(X_train)\n",
    "X_train = vec.transform(X_train)\n",
    "X_test = vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C=1000, penalty='l1', max_iter=500, multi_class='auto')\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1664, 206465), (417, 206465))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cnn_model\n",
    "\n",
    "# EMBEDDING\n",
    "MAX_NUM_WORDS  = 50000 #15000\n",
    "EMBEDDING_DIM  = 300\n",
    "MAX_SEQ_LENGTH = X_train.shape[1]# or 3200 #200\n",
    "USE_GLOVE      = False\n",
    "\n",
    "# MODEL\n",
    "FILTER_SIZES   = [3,4,5]\n",
    "FEATURE_MAPS   = [10,10,10]\n",
    "DROPOUT_RATE   = 0.5\n",
    "\n",
    "# LEARNING\n",
    "BATCH_SIZE     = 20\n",
    "NB_EPOCHS      = 40\n",
    "RUNS           = 5\n",
    "VAL_SIZE       = 0.2\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    model = cnn_model.build_cnn(\n",
    "            embedding_layer=emb_layer,\n",
    "            num_words=MAX_NUM_WORDS,\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            filter_sizes=FILTER_SIZES,\n",
    "            feature_maps=FEATURE_MAPS,\n",
    "            max_seq_length=MAX_SEQ_LENGTH,\n",
    "            dropout_rate=DROPOUT_RATE\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "            loss='binary_crossentropy',\n",
    "            optimizer=Adadelta(clipvalue=3),\n",
    "            metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CNN 0.0.1\n",
      "#############################################\n",
      "Embedding:    no pre-trained embedding\n",
      "Vocabulary size: 50000\n",
      "Embedding dim: 300\n",
      "Filter sizes: [3, 4, 5]\n",
      "Feature maps: [10, 10, 10]\n",
      "Max sequence: 206465\n",
      "#############################################\n",
      "Train on 1664 samples, validate on 333 samples\n",
      "Epoch 1/40\n"
     ]
    }
   ],
   "source": [
    "#import cnn_model\n",
    "#from sklearn.model_selection import StratifiedKFold\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "histories = []\n",
    "test_loss = []\n",
    "test_accs = []\n",
    "\n",
    "predicted_y = []\n",
    "expected_y = []\n",
    "\n",
    "#K = StratifiedKFold(n_splits=2)\n",
    "\n",
    "X_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "emb_layer = None\n",
    "#if USE_GLOVE:\n",
    "    #emb_layer = create_glove_embeddings()\n",
    "\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, \n",
    "                        epochs=NB_EPOCHS,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        verbose=1,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        callbacks=[#ModelCheckpoint('model-%i.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min'),\n",
    "                            ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, min_lr=0.01),\n",
    "                            EarlyStopping(monitor='val_loss', min_delta=0.1, patience=4, verbose=1)\n",
    "                        ])\n",
    "\n",
    "history = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=classes_name))\n",
    "print()\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "plot_confusion_matrix(cm, classes_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=classes_name))\n",
    "print()\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "plot_confusion_matrix(cm, classes_name)\n",
    "y_score = model.predict_proba(X_test)\n",
    "# ROC(y_test, y_score, n_classes, None, None, classes_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
