{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from Models.functions.plot import ROC, plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.functions.datasets import getDatasets\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import re, string\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "#from Models.functions.plot import ROC, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def labelEncoder(y):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y)\n",
    "\n",
    "    return (le.transform(y), len(le.classes_), list(le.classes_))\n",
    "\n",
    "def clean(doc):\n",
    "    \"\"\"\n",
    "    Cleaning a document by several methods:\n",
    "        - Lowercase\n",
    "        - Removing whitespaces\n",
    "        - Removing numbers\n",
    "        - Removing stopwords\n",
    "        - Removing punctuations\n",
    "        - Removing short words\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    \n",
    "    # Lowercase\n",
    "    # doc = doc.lower()    \n",
    "    # Remove HTML codes\n",
    "    doc = BeautifulSoup(doc).get_text()    \n",
    "    # Remove numbers\n",
    "    # doc = re.sub(r\"[0-9]+\", \"\", doc)\n",
    "    # remove HTML space code\n",
    "    # tokens = tokens.replace('&nbsp', string.whitespace)\n",
    "    # Split in tokens\n",
    "    tokens = doc.split()\n",
    "    # Remove Stopwords\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # Remove punctuation\n",
    "    tokens = [w.translate(str.maketrans('', '', string.punctuation)) for w in tokens]\n",
    "    # Tokens with less then two characters will be ignored\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'age'\n",
    "dataset_name = 'b5post'\n",
    "\n",
    "datasets = getDatasets(task,'df', dataset_name)\n",
    "for i in datasets.iterrows():\n",
    "\n",
    "    name = i[1]['dataset_name']\n",
    "    label = task\n",
    "    ds_path = i[1]['path']\n",
    "\n",
    "    # load training and test dataframes\n",
    "    training_path = ds_path + '/' + i[1]['training']        \n",
    "    #test_path = ds_path + '/' + i[1]['test']      \n",
    "\n",
    "    df_training = pd.read_csv(training_path)#, usecols=cols)        \n",
    "    #df_test = pd.read_csv(test_path)#, usecols=cols)        \n",
    "\n",
    "    df_training['text'] = df_training['text'].apply(clean)\n",
    "    #df_test['text'] = df_test['text'].apply(clean)\n",
    "    X = df_training['text'].values\n",
    "    y, n_classes, classes_name = labelEncoder(df_training[label].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(lines):\n",
    "    \"\"\"\n",
    "    Calculate the maximum document length\n",
    "    \"\"\"\n",
    "    return max([len(s.split()) for s in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text informations:\n",
      "max length: 13257 / min length: 5 / mean length: 1356 \n"
     ]
    }
   ],
   "source": [
    "max_length = max_length(X)\n",
    "\n",
    "result = [len(x.split()) for x in X]\n",
    "print('Text informations:')\n",
    "print('max length: %i / min length: %i / mean length: %i ' % (np.max(result),\n",
    "                                                                np.min(result),\n",
    "                                                                np.mean(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    \"\"\"\n",
    "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
    "    \n",
    "    get_top_n_words([\"I love Python\", \"Python is a language programming\", \"Hello world\", \"I love the world\"]) -> \n",
    "    [('python', 2),\n",
    "     ('world', 2),\n",
    "     ('love', 2),\n",
    "     ('hello', 1),\n",
    "     ('is', 1),\n",
    "     ('programming', 1),\n",
    "     ('the', 1),\n",
    "     ('language', 1)]\n",
    "    \"\"\"\n",
    "    vec = TfidfVectorizer().fit(corpus)\n",
    "    # vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "a18-20    151\n",
      "a23-25    149\n",
      "a28-61    113\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_training.groupby('age').size())\n",
    "c = df_training.groupby('age').size().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a18-20\n",
      "kkk 15.979267484563353\n",
      "pra 10.78384720354864\n",
      "dia 6.866689025860669\n",
      "vida 6.42851646423661\n",
      "amo 6.121366382072447\n",
      "amor 5.917618962370609\n",
      "tudo 5.689905327180669\n",
      "eu 5.453057261057598\n",
      "ser 5.372193072377578\n",
      "sempre 5.330079930652694\n",
      "--\n",
      "a23-25\n",
      "kkk 13.633582905704925\n",
      "pra 10.842666305988573\n",
      "dia 7.382414047442628\n",
      "the 6.332044976063848\n",
      "ser 6.31510427109672\n",
      "vai 5.265019628541888\n",
      "vida 5.088711823430247\n",
      "hoje 4.827417837207707\n",
      "não 4.7311203010830765\n",
      "tudo 4.68773141459932\n",
      "--\n",
      "a28-61\n",
      "kkk 8.91021131151838\n",
      "dia 6.6435723365180595\n",
      "pra 6.578095272590237\n",
      "ser 4.448005020599045\n",
      "vida 3.8935273526569536\n",
      "deus 3.7615819916111364\n",
      "hoje 3.728788939320164\n",
      "não 3.6258170792201794\n",
      "bom 3.5654900783899084\n",
      "todos 3.466517565373286\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_classes):\n",
    "    c1 = df_training[df_training[task] == c[i]]\n",
    "    print(classes_name[i])\n",
    "    text = [line.replace(\"\\n\", \"\") for line in c1.text]\n",
    "    common_words = get_top_n_words(text, 10)\n",
    "    for word, freq in common_words:\n",
    "        print(word, freq)\n",
    "    print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "vec = TfidfVectorizer().fit(X_train)\n",
    "X_train = vec.transform(X_train)\n",
    "X_test = vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "\n",
    "clf = LogisticRegressionCV(cv=5, max_iter=500, multi_class='auto')\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      a18-20       0.47      0.61      0.53        28\n",
      "      a23-25       0.57      0.47      0.52        34\n",
      "      a28-61       0.63      0.57      0.60        21\n",
      "\n",
      "   micro avg       0.54      0.54      0.54        83\n",
      "   macro avg       0.56      0.55      0.55        83\n",
      "weighted avg       0.55      0.54      0.54        83\n",
      "\n",
      "\n",
      "[[17 10  1]\n",
      " [12 16  6]\n",
      " [ 7  2 12]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=classes_name))\n",
    "print()\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "#plot_confusion_matrix(cm, classes_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((330, 47102), (83, 47102))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cnn_model\n",
    "\n",
    "# EMBEDDING\n",
    "MAX_NUM_WORDS  = 50000 #15000\n",
    "EMBEDDING_DIM  = 300\n",
    "MAX_SEQ_LENGTH = X_train.shape[1] or 3200 #200\n",
    "USE_GLOVE      = False\n",
    "\n",
    "# MODEL\n",
    "FILTER_SIZES   = [3,4,5]\n",
    "FEATURE_MAPS   = [10,10,10]\n",
    "DROPOUT_RATE   = 0.5\n",
    "\n",
    "# LEARNING\n",
    "BATCH_SIZE     = 20\n",
    "NB_EPOCHS      = 40\n",
    "RUNS           = 5\n",
    "VAL_SIZE       = 0.2\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    model = cnn_model.build_cnn(\n",
    "            embedding_layer=emb_layer,\n",
    "            num_words=MAX_NUM_WORDS,\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            filter_sizes=FILTER_SIZES,\n",
    "            feature_maps=FEATURE_MAPS,\n",
    "            max_seq_length=MAX_SEQ_LENGTH,\n",
    "            dropout_rate=DROPOUT_RATE\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "            loss='binary_crossentropy',\n",
    "            optimizer=Adadelta(clipvalue=3),\n",
    "            metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CNN 0.0.1\n",
      "#############################################\n",
      "Embedding:    using pre-trained embedding\n",
      "Vocabulary size: 50000\n",
      "Embedding dim: 300\n",
      "Filter sizes: [3, 4, 5]\n",
      "Feature maps: [10, 10, 10]\n",
      "Max sequence: 47102\n",
      "#############################################\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-185ef46af7c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-108fb0465fdb>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mfeature_maps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFEATURE_MAPS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SEQ_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDROPOUT_RATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rafael/drive/Models/baseline2/cnn_model.py\u001b[0m in \u001b[0;36mbuild_cnn\u001b[0;34m(embedding_layer, num_words, embedding_dim, filter_sizes, feature_maps, max_seq_length, dropout_rate)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mchannels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mx_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0memb_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0memb_layer\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "#import cnn_model\n",
    "#from sklearn.model_selection import StratifiedKFold\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "histories = []\n",
    "test_loss = []\n",
    "test_accs = []\n",
    "\n",
    "predicted_y = []\n",
    "expected_y = []\n",
    "\n",
    "#K = StratifiedKFold(n_splits=2)\n",
    "\n",
    "X_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "emb_layer = 'non-static'\n",
    "#if USE_GLOVE:\n",
    "    #emb_layer = create_glove_embeddings()\n",
    "\n",
    "\n",
    "cnn = create_model()\n",
    "\n",
    "model = KerasClassifier(build_fn=cnn)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=NB_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=0,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[#ModelCheckpoint('model-%i.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min'),\n",
    "               ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, min_lr=0.01),\n",
    "               EarlyStopping(monitor='val_loss', min_delta=0.1, patience=4, verbose=1)\n",
    "              ]\n",
    ")\n",
    "\n",
    "y_pred = model.predict(X_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=classes_name))\n",
    "print()\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "#plot_confusion_matrix(cm, classes_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
