{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from functions.models import \n",
    "from functions.datasets import getDatasets\n",
    "from functions.metrics import evaluator\n",
    "from functions.plot import ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__C': 1000.0,\n",
       " 'clf__penalty': 'l2',\n",
       " 'vect__max_df': 1,\n",
       " 'vect__max_features': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'pan13_en'\n",
    "task = 'age'\n",
    "\n",
    "def getBestParams(task, dataset_name):\n",
    "    \n",
    "    dataset_name = dataset_name.strip().lower()\n",
    "    task = task.strip().lower()\n",
    "    \n",
    "    # load excel params\n",
    "    baseline1 = pd.read_excel('./Reports/Reports.xlsx','baseline1')\n",
    "    baseline1['Task'] = baseline1['Task'].str.lower()\n",
    "    baseline1['Name'] = baseline1['Name'].str.lower()\n",
    "    \n",
    "    best_params = baseline1[(baseline1['Name'] == dataset_name) & (baseline1['Task'] == task)]\n",
    "    \n",
    "    max_features = best_params['max features'].values[0]\n",
    "    \n",
    "    model_params = {\n",
    "                    'vect__max_features': max_features if max_features != 'None' and not pd.isna(max_features) else None,\n",
    "                    'vect__max_df': best_params['max df'].values[0] if not pd.isna(best_params['max df'].values[0]) else 1,\n",
    "                    'clf__C': best_params['C'].values[0] if not pd.isna(best_params['C'].values[0]) else 1000.0, \n",
    "                    'clf__penalty': best_params['P'].values[0] if not pd.isna(best_params['P'].values[0]) else 'l2'\n",
    "                    }\n",
    "    \n",
    "    return model_params\n",
    "\n",
    "getBestParams(task, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelEncoder(y):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y)\n",
    "\n",
    "    return (le.transform(y), len(le.classes_), list(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    space = ' '    \n",
    "    for char in ['\\n','-', '...', '*', '/', '+', '\\\\']:\n",
    "        text = text.replace(char, space)\n",
    "        \n",
    "    text = text.replace(space*2, space)\n",
    "    text = text.replace(space*3, space)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform classification for each problem / task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, y, n_classes, classes_name, params):\n",
    "                \n",
    "    pipeline = Pipeline([       \n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),        \n",
    "        ('clf', LogisticRegression(verbose=1)),\n",
    "    ])\n",
    "    \n",
    "    # pipeline.set_params(**params)    \n",
    "    vect = TfidfVectorizer(max_features=params.get('vect__max_features'), max_df=params.get('vect__max_df'))\n",
    "    \n",
    "    K = StratifiedKFold(n_splits=10)\n",
    "    \n",
    "    t0 = time()\n",
    "    \n",
    "    predicted_y = []\n",
    "    expected_y = []    \n",
    "    score_y = []\n",
    "    \n",
    "    for train_index, test_index in K.split(X, y):\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "        X_train = vect.fit_transform(X_train)\n",
    "        X_test = vect.transform(X_test)\n",
    "        \n",
    "        clf = LogisticRegression(C=params.get('clf__C'), penalty=params.get('clf__penalty'), solver='liblinear')\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        predicted_y.extend(clf.predict(X_test))\n",
    "        expected_y.extend(y_test)\n",
    "        score_y.extend(clf.predict_proba(X_test))\n",
    "\n",
    "        ### get train score\n",
    "\n",
    "    # print(\"done in %0.2fs and %0.1fmin\" % ((time() - t0), ((time() - t0) / 60) ))\n",
    "    # print()\n",
    "    \n",
    "    report = pd.DataFrame(classification_report(expected_y, predicted_y, digits=5, target_names=classes_name, output_dict=True))\n",
    "    report = report.transpose()\n",
    "    \n",
    "    return (\n",
    "        report, \n",
    "        np.asarray(expected_y),\n",
    "        np.asarray(predicted_y),\n",
    "        np.asarray(score_y)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('polit', 'pan13_en')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'pan13_en'\n",
    "task = 'polit'\n",
    "task, dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == '__main__':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(task, dataset_name, output):\n",
    "    datasets = getDatasets(task,'df', dataset_name)\n",
    "    for i in datasets.iterrows():\n",
    "\n",
    "        name = i[1]['dataset_name']\n",
    "        label = task\n",
    "        ds_path = i[1]['path']\n",
    "\n",
    "        # load training and test dataframes\n",
    "        training_path = ds_path + '/' + i[1]['training']        \n",
    "\n",
    "        df_training = pd.read_csv(training_path)#, usecols=cols)        \n",
    "\n",
    "        df_training['text'] = df_training['text'].apply(clean)\n",
    "        X_train = df_training['text'].values\n",
    "        y_train, n_classes, classes_name = labelEncoder(df_training[label].values)\n",
    "\n",
    "        # del(df_training)\n",
    "\n",
    "        # print(\"Dataset: {0} and task: {1}\".format(name, label))\n",
    "\n",
    "        # print(\"n_classes: {0}\".format(n_classes))\n",
    "\n",
    "        params = getBestParams(task, dataset_name)\n",
    "        # print(\"params: \", params)\n",
    "\n",
    "        report, expected_y, predicted_y, score_y = model(X_train, y_train, n_classes, classes_name, params)\n",
    "\n",
    "        # get ROC\n",
    "        roc_c = ROC(expected_y, score_y, n_classes, task, dataset_name)    \n",
    "        try:\n",
    "            report['roc'] = list(roc_c.values()) + [roc_c['macro']] * 2\n",
    "        except:\n",
    "            print(roc_c.values())\n",
    "            report['roc'] = list(roc_c.values()) + [1] * 2\n",
    "            \n",
    "        # compute accuracy\n",
    "        accuracy = accuracy_score(expected_y, predicted_y)\n",
    "        report['accuracy'] = [accuracy] * 6\n",
    "        # compute confusion matrix\n",
    "        cm = pd.DataFrame(confusion_matrix(expected_y, predicted_y), columns=classes_name, index=classes_name)\n",
    "\n",
    "        directory = './Reports/' + task + '/' + dataset_name + '/'\n",
    "        report.to_csv(directory + 'report.csv')\n",
    "        cm.to_csv(directory + 'confusion_matrix.csv')    \n",
    "        \n",
    "        print(task, dataset_name, report)\n",
    "        \n",
    "        output.put('results for {0} and {1}'.format(task, dataset_name))\n",
    "        output.put(report.to_dict())\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import random\n",
    "import string\n",
    "\n",
    "random.seed(123)\n",
    "\n",
    "# Define an output queue\n",
    "output = mp.Queue()\n",
    "\n",
    "task_list = ['relig','polit','education','professional','region','TI']#,'gender','age']\n",
    "dataset_list = ['brmoral','b5post','esic','brblogset','enblogs','pan13_en','pan13_es','sms']\n",
    "\n",
    "args = []\n",
    "for task in task_list:\n",
    "    for ds in dataset_list:\n",
    "        d = getDatasets(task,'df', ds)\n",
    "        if len(d.values) > 0:\n",
    "            args.append([task, ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/rafael/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/rafael/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/rafael/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/rafael/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "education brmoral                                      f1-score  precision    recall  support  \\\n",
      "Básico + Superior incompleto         0.454183   0.448819  0.459677    124.0   \n",
      "Pós-graduação andamento ou completo  0.398104   0.392523  0.403846    104.0   \n",
      "Superior completo                    0.287234   0.296703  0.278351     97.0   \n",
      "micro avg                            0.387692   0.387692  0.387692    325.0   \n",
      "macro avg                            0.379841   0.379349  0.380625    325.0   \n",
      "weighted avg                         0.386410   0.385404  0.387692    325.0   \n",
      "\n",
      "                                          roc  accuracy  \n",
      "Básico + Superior incompleto         0.595209  0.387692  \n",
      "Pós-graduação andamento ou completo  0.579882  0.387692  \n",
      "Superior completo                    0.504205  0.387692  \n",
      "micro avg                            0.562087  0.387692  \n",
      "macro avg                            0.562087  0.387692  \n",
      "weighted avg                         0.562087  0.387692  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-2ef24beade8f>\", line 32, in run\n",
      "    report['roc'] = list(roc_c.values()) + [roc_c['macro']] * 2\n",
      "KeyError: 'macro'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-9-2ef24beade8f>\", line 34, in run\n",
      "    report['roc'] = list(roc_c.values()) + [1] * 2\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\", line 3119, in __setitem__\n",
      "    self._set_item(key, value)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\", line 3194, in _set_item\n",
      "    value = self._sanitize_column(key, value)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\", line 3391, in _sanitize_column\n",
      "    value = _sanitize_index(value, self.index, copy=False)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\", line 4001, in _sanitize_index\n",
      "    raise ValueError('Length of values does not match length of ' 'index')\n",
      "ValueError: Length of values does not match length of index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relig brmoral               f1-score  precision    recall  support  roc  accuracy\n",
      "r12           0.705570   0.545082  1.000000    133.0  0.5  0.545082\n",
      "r3            0.000000   0.000000  0.000000     90.0  0.5  0.545082\n",
      "r45           0.000000   0.000000  0.000000     21.0  0.5  0.545082\n",
      "micro avg     0.545082   0.545082  0.545082    244.0  0.5  0.545082\n",
      "macro avg     0.235190   0.181694  0.333333    244.0  0.5  0.545082\n",
      "weighted avg  0.384594   0.297114  0.545082    244.0  0.5  0.545082\n",
      "polit brmoral               f1-score  precision    recall  support       roc  accuracy\n",
      "p12           0.588745   0.571429  0.607143    112.0  0.753563  0.523077\n",
      "p3            0.532258   0.519685  0.545455    121.0  0.647667  0.523077\n",
      "p45           0.421053   0.455696  0.391304     92.0  0.684176  0.523077\n",
      "micro avg     0.523077   0.523077  0.523077    325.0  0.697439  0.523077\n",
      "macro avg     0.514018   0.515603  0.514634    325.0  0.697439  0.523077\n",
      "weighted avg  0.520244   0.519403  0.523077    325.0  0.697439  0.523077\n",
      "relig b5post               f1-score  precision    recall  support       roc  accuracy\n",
      "r12           0.686916   0.581028  0.840000    175.0  0.684939  0.551136\n",
      "r3            0.043011   0.142857  0.025316     79.0  0.531877  0.551136\n",
      "r45           0.491803   0.529412  0.459184     98.0  0.694962  0.551136\n",
      "micro avg     0.551136   0.551136  0.551136    352.0  0.639576  0.551136\n",
      "macro avg     0.407243   0.417766  0.441500    352.0  0.639576  0.551136\n",
      "weighted avg  0.488082   0.468318  0.551136    352.0  0.639576  0.551136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/rafael/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "Process Process-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-2ef24beade8f>\", line 32, in run\n",
      "    report['roc'] = list(roc_c.values()) + [roc_c['macro']] * 2\n",
      "KeyError: 'macro'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-9-2ef24beade8f>\", line 34, in run\n",
      "    report['roc'] = list(roc_c.values()) + [1] * 2\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\", line 3119, in __setitem__\n",
      "    self._set_item(key, value)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\", line 3194, in _set_item\n",
      "    value = self._sanitize_column(key, value)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\", line 3391, in _sanitize_column\n",
      "    value = _sanitize_index(value, self.index, copy=False)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\", line 4001, in _sanitize_index\n",
      "    raise ValueError('Length of values does not match length of ' 'index')\n",
      "ValueError: Length of values does not match length of index\n",
      "/home/rafael/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/rafael/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Setup a list of processes that we want to run\n",
    "processes = [mp.Process(target=run, args=(x[0], x[1], output)) for x in args]\n",
    "\n",
    "# Run processes\n",
    "for p in processes:\n",
    "    p.start()\n",
    "\n",
    "# Exit the completed processes\n",
    "for p in processes:\n",
    "    p.join()\n",
    "\n",
    "# Get process results from the output queue\n",
    "results = [output.get() for p in processes]\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
