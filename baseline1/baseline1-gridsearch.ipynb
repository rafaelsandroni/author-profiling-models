{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/rafael/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/rafael/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from functions.models import \n",
    "from Models.functions.datasets import getDatasets, loadTrainTest\n",
    "from Models.functions.metrics import evaluator\n",
    "from Models.functions.plot import ROC, plot_confusion_matrix\n",
    "# from Models.functions.preprocessing import clean\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from collections import Counter\n",
    "\n",
    "# Synthetic Minority Oversampling Technique (SMOTE)\n",
    "def oversampling(X, y):    \n",
    "    X_resampled, y_resampled = SMOTE(random_state=42).fit_resample(X, y)    \n",
    "    #return X, y\n",
    "    return X_resampled, y_resampled        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__C': 1.0,\n",
       " 'clf__penalty': 'l2',\n",
       " 'vect__max_df': 1,\n",
       " 'vect__max_features': None}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'brmoral'\n",
    "task = 'smscorpus'\n",
    "\n",
    "def getBestParams(task, dataset_name):\n",
    "    baseline = 'baseline1'\n",
    "    dataset_name = dataset_name.strip().lower()\n",
    "    task = task.strip().lower()\n",
    "    \n",
    "    # load excel params\n",
    "    baseline1 = pd.read_excel('./Reports_v1/Reports.xlsx', baseline)\n",
    "    \n",
    "    baseline1['Task'] = baseline1['Task'].str.lower()\n",
    "    baseline1['Name'] = baseline1['Name'].str.lower()\n",
    "    \n",
    "    best_params = baseline1[(baseline1['Name'] == dataset_name) & (baseline1['Task'] == task)]\n",
    "    \n",
    "    if len(best_params) < 1: return {\n",
    "                    'vect__max_features': None,\n",
    "                    'vect__max_df': 1,\n",
    "                    'clf__C': 1.0,\n",
    "                    'clf__penalty': 'l2'\n",
    "                    }\n",
    "    \n",
    "    max_features = best_params['max features'].values[0]\n",
    "    \n",
    "    model_params = {\n",
    "                    'vect__max_features': max_features if max_features != 'None' and not pd.isnull(max_features) else None,\n",
    "                    'vect__max_df': best_params['max df'].values[0] if not pd.isnull(best_params['max df'].values[0]) else 1,\n",
    "                    'clf__C': best_params['C'].values[0] if not pd.isnull(best_params['C'].values[0]) else 1000.0, \n",
    "                    'clf__penalty': best_params['P'].values[0] if not pd.isnull(best_params['P'].values[0]) else 'l2'\n",
    "                    }\n",
    "    \n",
    "    return model_params\n",
    "\n",
    "getBestParams(task, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelEncoder(y):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y)\n",
    "\n",
    "    return (le.transform(y), len(le.classes_), list(le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform classification for each problem / task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, X_test, y_train, y_test, n_classes, classes_name, params):\n",
    "    \n",
    "    t0 = time()\n",
    "    \n",
    "    predicted_y = []\n",
    "    expected_y = []    \n",
    "    score_y = []\n",
    "\n",
    "    params_grid = dict(\n",
    "            clf__C = np.linspace(1e-4, 1e4, num=8),\n",
    "            clf__penalty = ['l1','l2'],\n",
    "            vect__max_df = [0.8, 0.9, 1.0],            \n",
    "            vect__max_features = [None, 1000, 3000, 9000],\n",
    "            vect__stop_words= [en_stopwords, None]\n",
    "    )                               \n",
    "            # vect__ngram_range = [(1, 1), (3, 5)],                    \n",
    "                \n",
    "    pipeline = Pipeline([\n",
    "        ('vect', TfidfVectorizer()),        \n",
    "        #('smote', SMOTE()),\n",
    "        ('clf', LogisticRegression(verbose=1)),\n",
    "    ])\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, params_grid, scoring='accuracy')\n",
    "    \n",
    "    #print('best params', best_model.best_params_)\n",
    "    #print('best scores', best_model.best_score_)\n",
    "    print(\"Performing grid search...\")    \n",
    "    print(\"Pipeline steps:\", [name for name, _ in pipeline.steps])    \n",
    "    t0 = time()\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"done in %0.2fs and %0.1fmin\" % ((time() - t0), ((time() - t0) / 60) ))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(params_grid.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    y_score = grid_search.decision_function(X_test)\n",
    "    \n",
    "    print()\n",
    "    y_pred = grid_search.predict(X_test) \n",
    "    print()\n",
    "\n",
    "    predicted_y.extend(y_pred)\n",
    "    expected_y.extend(y_test)\n",
    "    score_y.extend(y_score)\n",
    "\n",
    "    ### get train score\n",
    "\n",
    "    # print(\"done in %0.2fs and %0.1fmin\" % ((time() - t0), ((time() - t0) / 60) ))\n",
    "    # print()\n",
    "    \n",
    "    report = pd.DataFrame(classification_report(expected_y, predicted_y, digits=5, target_names=classes_name, output_dict=True))\n",
    "    report = report.transpose()\n",
    "    \n",
    "    return (\n",
    "        report, \n",
    "        np.asarray(expected_y),\n",
    "        np.asarray(predicted_y),\n",
    "        np.asarray(score_y)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(task, dataset_name, root = None):    \n",
    "    datasets = getDatasets(task,'df', dataset_name, root)\n",
    "    for i in datasets.iterrows():\n",
    "\n",
    "        name = i[1]['dataset_name']\n",
    "        label = task\n",
    "        ds_path = i[1]['path']\n",
    "\n",
    "        # load training and test dataframes\n",
    "        training_path = ds_path + '/' + i[1]['training']        \n",
    "        test_path = ds_path + '/' + i[1]['test']        \n",
    "\n",
    "        df_training = pd.read_csv(training_path)#, usecols=cols)                \n",
    "        df_test = pd.read_csv(test_path)#, usecols=cols)                \n",
    "        #df_training['text'] = df_training['text'].apply(clean)\n",
    "        \n",
    "        X_train = df_training['text'].values\n",
    "        y_train, n_classes, classes_name = labelEncoder(df_training[label].values)\n",
    "        \n",
    "        X_test = df_test['text'].values\n",
    "        y_test, n_classes, classes_name = labelEncoder(df_test[label].values)\n",
    "\n",
    "        params = None\n",
    "        \n",
    "        report, expected_y, predicted_y, score_y = model(X_train, X_test, y_train, y_test, n_classes, classes_name, params)\n",
    "\n",
    "        # get ROC\n",
    "        roc_c = ROC(expected_y, score_y, n_classes, task, dataset_name, classes_name)\n",
    "        report['roc'] = list(roc_c.values()) + [roc_c['macro']] * 2\n",
    "\n",
    "        # compute accuracy\n",
    "        accuracy = accuracy_score(expected_y, predicted_y)\n",
    "        report['accuracy'] = [accuracy] * (n_classes + 3)\n",
    "\n",
    "        # compute confusion matrix\n",
    "        c_matrix = confusion_matrix(expected_y, predicted_y)\n",
    "        plot_confusion_matrix(c_matrix, classes_name, task, dataset_name, True)\n",
    "        cm = pd.DataFrame(c_matrix, columns=classes_name, index=classes_name)\n",
    "\n",
    "        directory = './Reports/' + task + '/' + dataset_name + '/'\n",
    "        report.to_csv(directory + 'report.csv')\n",
    "        cm.to_csv(directory + 'confusion_matrix.csv')    \n",
    "\n",
    "        print(task, dataset_name, report)\n",
    "        print(cm)\n",
    "\n",
    "        # output.put('results for {0} and {1}'.format(task, dataset_name))\n",
    "        # output.put(report.to_dict())\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from /home/rafael/GDrive/Data/Dataframe/\n",
      "Performing grid search...\n",
      "Pipeline steps: ['vect', 'clf']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]"
     ]
    }
   ],
   "source": [
    "run(\"gender\", \"smscorpus\", \"/home/rafael/GDrive/Data/Dataframe/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-9515a5e53921>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/rafael/GDrive/Data/Dataframe/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"en\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mloadTrainTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/Models-1.0-py3.5.egg/Models/functions/datasets.py\u001b[0m in \u001b[0;36mloadTrainTest\u001b[0;34m(task, dataset_name, root, lang)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"df\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mtrain_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{0}_{1}_training_{3}.csv\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mtest_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{0}_{1}_test_{3}.csv\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "task = \"gender\"\n",
    "dataset_name = \"smscorpus\"\n",
    "root = \"/home/rafael/GDrive/Data/Dataframe/\"\n",
    "lang = \"en\"\n",
    "loadTrainTest(task, dataset_name, root, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import random\n",
    "import string\n",
    "\n",
    "random.seed(123)\n",
    "\n",
    "# Define an output queue\n",
    "output = mp.Queue()\n",
    "\n",
    "task_list = ['relig','polit','education','professional','region','TI','gender','age']\n",
    "dataset_list = ['brmoral','b5post','esic','brblogset','enblogs','pan13_en','pan13_es']\n",
    "\n",
    "args = []\n",
    "for task in task_list:\n",
    "    for ds in dataset_list:\n",
    "        d = getDatasets(task,'df', ds)\n",
    "        if len(d.values) > 0:\n",
    "            args.append([task, ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Setup a list of processes that we want to run\n",
    "processes = [mp.Process(target=run, args=(x[0], x[1], output)) for x in args]\n",
    "\n",
    "# Run processes\n",
    "for p in processes:\n",
    "    p.start()\n",
    "\n",
    "# Exit the completed processes\n",
    "for p in processes:\n",
    "    p.join()\n",
    "\n",
    "# Get process results from the output queue\n",
    "results = [output.get() for p in processes]\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
