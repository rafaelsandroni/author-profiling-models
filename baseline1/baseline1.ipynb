{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from functions.models import \n",
    "from Models.functions.datasets import getDatasets, loadTrainTest\n",
    "from Models.functions.metrics import evaluator\n",
    "from Models.functions.plot import ROC, plot_confusion_matrix\n",
    "from Models.functions.preprocessing import clean, labelEncoder\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from collections import Counter\n",
    "\n",
    "# Synthetic Minority Oversampling Technique (SMOTE)\n",
    "def oversampling(X, y):    \n",
    "    X_resampled, y_resampled = SMOTE(random_state=42).fit_resample(X, y)    \n",
    "    #return X, y\n",
    "    return X_resampled, y_resampled        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestParams(task, dataset_name):\n",
    "    baseline = 'baseline1'\n",
    "    dataset_name = dataset_name.strip().lower()\n",
    "    task = task.strip().lower()\n",
    "    \n",
    "    # load excel params\n",
    "    baseline1 = pd.read_excel('./Reports_v1/Reports.xlsx', baseline)\n",
    "    \n",
    "    baseline1['Task'] = baseline1['Task'].str.lower()\n",
    "    baseline1['Name'] = baseline1['Name'].str.lower()\n",
    "    \n",
    "    best_params = baseline1[(baseline1['Name'] == dataset_name) & (baseline1['Task'] == task)]\n",
    "    \n",
    "    \n",
    "    if len(best_params) < 1: return dict(\n",
    "                clf__C =  1428.5715142857143,\n",
    "                clf__penalty =  'l2',\n",
    "                vect__max_df =  0.8,\n",
    "                vect__max_features =  1000,\n",
    "                vect__stop_words = None)\n",
    "    \n",
    "    max_features = best_params['max features'].values[0]\n",
    "    \n",
    "    model_params = {\n",
    "                    'vect__max_features': max_features if max_features != 'None' and not pd.isnull(max_features) else None,\n",
    "                    'vect__max_df': best_params['max df'].values[0] if not pd.isnull(best_params['max df'].values[0]) else 1,\n",
    "                    'clf__C': best_params['C'].values[0] if not pd.isnull(best_params['C'].values[0]) else 1000.0, \n",
    "                    'clf__penalty': best_params['P'].values[0] if not pd.isnull(best_params['P'].values[0]) else 'l2'\n",
    "                    }\n",
    "    \n",
    "    return model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform classification for each problem / task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, y, n_classes, classes_name, params):\n",
    "    \n",
    "    # pipeline.set_params(**params)    \n",
    "    vect = TfidfVectorizer(max_features=params.get('vect__max_features'), max_df=params.get('vect__max_df'))\n",
    "    \n",
    "    K = StratifiedKFold(n_splits=10)\n",
    "    \n",
    "    t0 = time()\n",
    "    \n",
    "    predicted_y = []\n",
    "    expected_y = []    \n",
    "    score_y = []\n",
    "    \n",
    "    for train_index, test_index in K.split(X, y):\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "        X_train = vect.fit_transform(X_train)\n",
    "        X_test = vect.transform(X_test)\n",
    "        \n",
    "        X_train, y_train = oversampling(X_train, y_train)\n",
    "        X_test, y_test = oversampling(X_test, y_test)\n",
    "\n",
    "        clf = LogisticRegression(C=params.get('clf__C'), penalty=params.get('clf__penalty'), solver='liblinear')\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        predicted_y.extend(clf.predict(X_test))\n",
    "        expected_y.extend(y_test)\n",
    "        score_y.extend(clf.predict_proba(X_test))\n",
    "\n",
    "        ### get train score\n",
    "\n",
    "    # print(\"done in %0.2fs and %0.1fmin\" % ((time() - t0), ((time() - t0) / 60) ))\n",
    "    # print()\n",
    "    \n",
    "    report = pd.DataFrame(classification_report(expected_y, predicted_y, digits=5, target_names=classes_name, output_dict=True))\n",
    "    report = report.transpose()\n",
    "    \n",
    "    return (\n",
    "        report, \n",
    "        np.asarray(expected_y),\n",
    "        np.asarray(predicted_y),\n",
    "        np.asarray(score_y)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(task, dataset_name, root, lang):    \n",
    "    \n",
    "    X, _, y, _ = loadTrainTest(task, dataset_name, root, lang)\n",
    "    y, n_classes, classes_name = labelEncoder(y.values)\n",
    "\n",
    "    params = getBestParams(task, dataset_name)    \n",
    "    print(\"params: \", params)\n",
    "\n",
    "    report, expected_y, predicted_y, score_y = model(X, y, n_classes, classes_name, params)\n",
    "\n",
    "    # get ROC\n",
    "    roc_c = ROC(expected_y, score_y, n_classes, task, dataset_name, classes_name)\n",
    "    report['roc'] = list(roc_c.values()) + [roc_c['macro']] * 2\n",
    "\n",
    "    # compute accuracy\n",
    "    accuracy = accuracy_score(expected_y, predicted_y)\n",
    "    report['accuracy'] = [accuracy] * (n_classes + 3)\n",
    "\n",
    "    # compute confusion matrix\n",
    "    c_matrix = confusion_matrix(expected_y, predicted_y)\n",
    "    plot_confusion_matrix(c_matrix, classes_name, task, dataset_name, True)\n",
    "    cm = pd.DataFrame(c_matrix, columns=classes_name, index=classes_name)\n",
    "\n",
    "    directory = './Reports/' + task + '/' + dataset_name + '/'\n",
    "    report.to_csv(directory + 'report.csv')\n",
    "    cm.to_csv(directory + 'confusion_matrix.csv')    \n",
    "\n",
    "    print(task, dataset_name, report)\n",
    "    print(cm)\n",
    "\n",
    "    # output.put('results for {0} and {1}'.format(task, dataset_name))\n",
    "    # output.put(report.to_dict())\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6bba43d9935a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gender\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"smscorpus\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/home/rafael/GDrive/Data/Dataframe/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-e82eeb6789b9>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(task, dataset_name, root, lang)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadTrainTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetBestParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "run(\"gender\", \"smscorpus\", \"/home/rafael/GDrive/Data/Dataframe/\", 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"gender\"\n",
    "dataset_name = \"smscorpus\"\n",
    "root = \"/home/rafael/GDrive/Data/Dataframe/\"\n",
    "lang = \"en\"\n",
    "X, _, _, _ = loadTrainTest(task, dataset_name, root, lang)\n",
    "\n",
    "X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import random\n",
    "import string\n",
    "\n",
    "random.seed(123)\n",
    "\n",
    "# Define an output queue\n",
    "output = mp.Queue()\n",
    "\n",
    "task_list = ['relig','polit','education','professional','region','TI','gender','age']\n",
    "dataset_list = ['brmoral','b5post','esic','brblogset','enblogs','pan13_en','pan13_es']\n",
    "\n",
    "args = []\n",
    "for task in task_list:\n",
    "    for ds in dataset_list:\n",
    "        d = getDatasets(task,'df', ds)\n",
    "        if len(d.values) > 0:\n",
    "            args.append([task, ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Setup a list of processes that we want to run\n",
    "processes = [mp.Process(target=run, args=(x[0], x[1], output)) for x in args]\n",
    "\n",
    "# Run processes\n",
    "for p in processes:\n",
    "    p.start()\n",
    "\n",
    "# Exit the completed processes\n",
    "for p in processes:\n",
    "    p.join()\n",
    "\n",
    "# Get process results from the output queue\n",
    "results = [output.get() for p in processes]\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
